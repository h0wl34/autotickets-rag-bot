{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6074cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e178cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get texts from text cols\n",
    "# a = df['QUESTION_clean']\n",
    "# b = df['TITLE']\n",
    "# c = df['ANSWER_clean']\n",
    "\n",
    "# a.to_csv('../data/question_texts.csv', index=False)\n",
    "# b.to_csv('../data/title_texts.csv', index=False)\n",
    "# c.to_csv('../data/answer_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b987f",
   "metadata": {},
   "source": [
    "### Сплит данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc25eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем стратифицировать по всем таргетам сразу\n",
    "df['stratify_key'] = df['SUBCATEGORY_clean'].astype(str) + '_' + df['PRIORITY'].astype(str) + df['AVARIYA'].astype(str)\n",
    "counts = df['stratify_key'].value_counts()\n",
    "rare_classes = counts[counts < 20].index\n",
    "df['stratify_key'] = df['stratify_key'].replace(rare_classes, 'OTHER')\n",
    "df['stratify_key'].value_counts()\n",
    "\n",
    "all_ids = df.index.values\n",
    "train_val_ids, test_ids = train_test_split(\n",
    "    all_ids,\n",
    "    test_size=0.10,\n",
    "    random_state=42,\n",
    "    stratify=df['stratify_key'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Subset stratify_key to the train_val_ids\n",
    "train_val_stratify = df.loc[train_val_ids, 'stratify_key']\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    train_val_ids,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=train_val_stratify,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "np.save(\"../data/splits/train.npy\", train_ids)\n",
    "np.save(\"../data/splits/val.npy\", val_ids)\n",
    "np.save(\"../data/splits/test.npy\", test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acce727",
   "metadata": {},
   "source": [
    "Добавим выборку поменьше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd5cdf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sizes - Train: 539850, Val: 95268, Test: 70569\n",
      "Subsampled sizes - Train: 179950, Val: 31756, Test: 23523\n"
     ]
    }
   ],
   "source": [
    "def subsample_stratified(ids, stratify_column, fraction=1/3, random_state=42):\n",
    "    \"\"\"Subsample while maintaining stratification\"\"\"\n",
    "    if fraction >= 1.0:\n",
    "        return ids\n",
    "    \n",
    "    # Get the stratify values for these IDs\n",
    "    stratify_values = stratify_column.loc[ids]\n",
    "    \n",
    "    # Perform stratified sampling\n",
    "    subsampled_ids, _ = train_test_split(\n",
    "        ids,\n",
    "        train_size=fraction,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_values,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return subsampled_ids\n",
    "\n",
    "# Subsample each split\n",
    "train_ids_subsampled = subsample_stratified(train_ids, df['stratify_key'], fraction=1/3)\n",
    "val_ids_subsampled = subsample_stratified(val_ids, df['stratify_key'], fraction=1/3)\n",
    "test_ids_subsampled = subsample_stratified(test_ids, df['stratify_key'], fraction=1/3)\n",
    "\n",
    "print(f\"Original sizes - Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
    "print(f\"Subsampled sizes - Train: {len(train_ids_subsampled)}, Val: {len(val_ids_subsampled)}, Test: {len(test_ids_subsampled)}\")\n",
    "\n",
    "np.save(\"../data/splits/train_small.npy\", train_ids_subsampled)\n",
    "np.save(\"../data/splits/val_small.npy\", val_ids_subsampled)\n",
    "np.save(\"../data/splits/test_small.npy\", test_ids_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e56b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AVARIYA\n",
       "Нет    531302\n",
       "Да       8548\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[train_ids]['AVARIYA'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1d4a2",
   "metadata": {},
   "source": [
    ">Вроде все классы есть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc291ad4",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331bf05",
   "metadata": {},
   "source": [
    "### Текстовые фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adeaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using\n",
    "# # TF-ID\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     max_features=8000,  # можно увеличить для точности\n",
    "#     ngram_range=(1,2),\n",
    "#     min_df=5\n",
    "# )\n",
    "\n",
    "# # Используем train split чтобы не было утечки\n",
    "# tfidf.fit(df.loc[train_ids, 'QUESTION_FULL'])\n",
    "# X_question_full = tfidf.transform(df['QUESTION_FULL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7becc85",
   "metadata": {},
   "source": [
    "Эмбеддинги для всех текстовых фич вычислили отдельно на сервере (bge_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4740fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_sentence_embeddings(df, column, model_path, save_path=None):\n",
    "#     model = SentenceTransformer(model_path)\n",
    "#     sentences = df[column].tolist()\n",
    "#     embeddings = model.encode(sentences, batch_size=64, show_progress_bar=True)\n",
    "    \n",
    "#     if save_path:\n",
    "#         np.savez_compressed(save_path, embeddings)\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_sentence_embeddings(df, 'QUESTION_FULL', model_path='../models/embeddings/deepvk_USER-bge-m3', save_path='../data/features/question_bge_m3.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce57b23",
   "metadata": {},
   "source": [
    "### Категориальные фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc655232",
   "metadata": {},
   "source": [
    "Закодируем s_name, reg, source с OE.\n",
    "\n",
    "Когда добавим новые классы, они будут маппиться в эмбеддинг с номером cardinality+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids again\n",
    "train_ids = np.load('../data/splits/train.npy')\n",
    "test_ids = np.load('../data/splits/test.npy')\n",
    "val_ids = np.load('../data/splits/val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2eb743da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220feab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ord_encoders = {}\n",
    "cat_cols = ['S_NAME', 'REG', 'SOURCE']\n",
    "# cat_cardinalities = {}\n",
    "\n",
    "# cat features cardinalities for train configs\n",
    "print(df[cat_cols].nunique())\n",
    "\n",
    "for col in cat_cols:\n",
    "    oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    oe.fit(df.loc[train_ids, [col]]) \n",
    "    df[col + '_enc'] = oe.transform(df[[col]]).flatten()  # Transform and flatten to 1D\n",
    "    ord_encoders[col] = oe\n",
    "    \n",
    "    # cardinality = len(oe.categories_[0])+1\n",
    "    # cat_cardinalities[col] = cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d4df3",
   "metadata": {},
   "source": [
    "Добавим категориальные фичи, выделенные плейсхолдерами из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "cf876c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns from preprocessing.ipynb\n",
    "SENSITIVE_PATTERNS = {\n",
    "    \"EMAIL\": r'\\b[A-Za-z0-9._%+-]+@(?:[A-Za-z0-9-]+\\.)+[A-Za-z]{2,}\\b|<\".+?\"@[^>]+>',\n",
    "    \"INN\": r'(?i)инн\\s*\\d{5,12}',\n",
    "    \"PHONE\": r'\\b(?:\\+7|8)?[\\s-]?\\(?\\d{3,4}\\)?[\\s-]?\\d{2,3}[\\s-]?\\d{2}[\\s-]?\\d{2}(?:\\s*\\(доб\\.\\s*\\d+\\))?\\b',\n",
    "    # \"PHONE_EXT\": r'\\b8\\s?\\d{3,4}[\\s-]?\\d{2}[\\s-]?\\d{2}[\\s-]?\\d{2}(?:\\s*\\(доб\\.\\s*\\d+\\))?',\n",
    "    \"VIN\": r'\\b[A-HJ-NPR-Z0-9]{17}\\b',\n",
    "    \"INCEDENT\": r'\\bIM\\d{8-12}\\b',\n",
    "    \"REG_NUMBER\": r'\\b[АВЕКМНОРСТУХ]{1,3}\\d{3,4}[АВЕКМНОРСТУХ]{2}\\b',\n",
    "    \"CASE_NO\": r'\\b(?:[CcSs]D|T)\\d{6,10}\\b',\n",
    "    \"APPEAL_NO\": r'\\b\\d/\\d{9,12}\\b',\n",
    "    \"DOC_NO\": r'№\\s?\\d{1,6}([/\\\\-]?\\d{1,6})?([А-ЯA-Z])?',\n",
    "    \"LONG_ID\": r'\\b\\d{9,}\\b|\\b[a-f0-9]{32,128}\\b|(?:UID|GlndID|GUID)[: ]?[0-9a-fA-F\\-]{16,128}',\n",
    "    \"IP\": r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "    \"DATE\": r'\\b\\d{2}[./-]\\d{2}[./-]\\d{2,4}(?:\\s?г\\.?)?',\n",
    "    \"FIO\": r'\\b[А-ЯЁ][а-яё]+ [А-ЯЁ]\\.[А-ЯЁ]\\.(?=[\\s,.)/]|$)',\n",
    "    \"USERNAME\": r'\\b[a-zA-Z][a-zA-Z0-9]{3,15}\\d\\b',\n",
    "    \"TOKEN\": r'\\b[a-f0-9]{16,64}\\b|\\b(?=.*[A-Z])(?=.*[a-z])(?=.*\\d)[A-Za-z0-9]{12,64}\\b',\n",
    "    \"URL\": r'https?://[^\\s]+',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5b4974eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sensitive_flags(text: str) -> dict:\n",
    "    \"\"\"Возвращает булевы фичи для классификаторов\"\"\"\n",
    "    flags = {f\"HAS_{k}\": 0 for k in SENSITIVE_PATTERNS.keys()}\n",
    "    \n",
    "    if not isinstance(text, str) or text.strip() == \"\" or text == \"[NO_TEXT]\":\n",
    "        flags[\"HAS_TEXT\"] = 0\n",
    "        return flags\n",
    "    \n",
    "    for placeholder in SENSITIVE_PATTERNS.keys():\n",
    "        if placeholder in text:\n",
    "            flags[f\"HAS_{placeholder}\"] = 1\n",
    "    \n",
    "    # Флаг наличия текста\n",
    "    flags[\"HAS_TEXT\"] = 1\n",
    "    \n",
    "    return flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3565937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_df = df['QUESTION_clean'].apply(extract_sensitive_flags).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82e49e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cols = flags_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c98bce83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S_NAME_enc', 'REG_enc', 'SOURCE_enc', 'HAS_EMAIL', 'HAS_INN',\n",
       "       'HAS_PHONE', 'HAS_VIN', 'HAS_INCEDENT', 'HAS_REG_NUMBER',\n",
       "       'HAS_CASE_NO', 'HAS_APPEAL_NO', 'HAS_DOC_NO', 'HAS_LONG_ID',\n",
       "       'HAS_IP', 'HAS_DATE', 'HAS_FIO', 'HAS_USERNAME', 'HAS_TOKEN',\n",
       "       'HAS_URL', 'HAS_TEXT'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols_enc = [f'{col}_enc' for col in cat_cols]\n",
    "# final_cat_cols = np.concatenate((cat_cols_enc, flag_cols))\n",
    "# final_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c004b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_oe = df[cat_cols_enc].values\n",
    "X_cat_bin = flags_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aa8aa701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((705687, 3), (705687, 17))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat_oe.shape, X_cat_bin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046d7f1",
   "metadata": {},
   "source": [
    "### Временные фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59d4c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['OPEN_TIME_', 'RESOLVE_TIME_', 'CLOSE_TIME_', 'ATC_NEXT_BREACH_']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Encoding cyclical features\n",
    "hour = df['OPEN_TIME_'].dt.hour\n",
    "df['hour_sin'] = np.sin(hour * (2. * np.pi / 24.))\n",
    "df['hour_cos'] = np.cos(hour * (2. * np.pi / 24.))\n",
    "\n",
    "day_of_week = df['OPEN_TIME_'].dt.dayofweek\n",
    "df['day_of_week_sin'] = np.sin(day_of_week * (2. * np.pi / 7.))\n",
    "df['day_of_week_cos'] = np.cos(day_of_week * (2. * np.pi / 7.))\n",
    "\n",
    "df['is_weekend'] = (day_of_week >= 5).astype(int)\n",
    "\n",
    "day_of_month = df['OPEN_TIME_'].dt.day\n",
    "df['day_of_month_sin'] = np.sin(day_of_month * (2. * np.pi / 31.))\n",
    "df['day_of_month_cos'] = np.cos(day_of_month * (2. * np.pi / 31.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b6cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_time = df[['day_of_week_sin', 'day_of_week_cos', 'hour_sin', 'hour_cos', 'day_of_month_sin', 'day_of_month_cos', 'is_weekend']].fillna(0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0335d40",
   "metadata": {},
   "source": [
    "### Таргеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99156710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVARIYA_enc'] = df['AVARIYA'].map({'Да':1, 'Нет':0})\n",
    "df['PRIORITY_enc'] = df['PRIORITY'].map({2: 0, 3: 1}).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd8f77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df.loc[train_ids, 'SUBCATEGORY_clean'])\n",
    "classes = le.classes_\n",
    "df[f'SUBCATEGORY_clean_processed'] = df['SUBCATEGORY_clean'].apply(lambda x: np.where(classes == x)[0][0] if x in classes else -1)\n",
    "df['SUBCATEGORY_clean_enc'] = le.transform(df['SUBCATEGORY_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c59f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['SUBCATEGORY_clean_enc', 'PRIORITY_enc', 'AVARIYA_enc']\n",
    "y = df[target_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49c3ec",
   "metadata": {},
   "source": [
    "### Сохраняем метаданные\n",
    "Сохраним размерности категориальных фич. Бинарные флаги будут подаваться в модель отдельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9f1bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_meta = {}\n",
    "for target in target_cols:\n",
    "    y_target = y[target].values\n",
    "    classes = np.unique(y_target)\n",
    "\n",
    "    # cardinality\n",
    "    cardinality = len(classes)\n",
    "    \n",
    "    # class weights (balanced)\n",
    "    if cardinality > 1:  # только для категориальных\n",
    "        if (cardinality == 2):\n",
    "            count0 = (y_target == 0).sum()\n",
    "            count1 = (y_target == 1).sum()\n",
    "            pos_weight = count0 / count1   # if we want to use the ratio\n",
    "            \n",
    "            targets_meta[target] = {\n",
    "                \"classes\": classes.tolist(),\n",
    "                \"cardinality\": cardinality,\n",
    "                \"pos_weight\": pos_weight\n",
    "            }\n",
    "        else:\n",
    "            weights = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=classes,\n",
    "                y=y_target\n",
    "            )\n",
    "            \n",
    "            targets_meta[target] = {\n",
    "                \"classes\": classes.tolist(),\n",
    "                \"cardinality\": cardinality,\n",
    "                \"class_weights\": weights.tolist()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc6ef29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SUBCATEGORY_clean_enc': {'classes': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13],\n",
       "  'cardinality': 14,\n",
       "  'class_weights': [4.478561908992829,\n",
       "   0.19927264286646143,\n",
       "   0.47579515282765206,\n",
       "   54.493204633204634,\n",
       "   14.09964035964036,\n",
       "   11.429980563654032,\n",
       "   7.123546394250182,\n",
       "   3.3532606629666235,\n",
       "   9.020439206462829,\n",
       "   245.88397212543555,\n",
       "   27.74144979951254,\n",
       "   13.349103359564165,\n",
       "   0.1729794587704677,\n",
       "   29.035837722185647]},\n",
       " 'PRIORITY_enc': {'classes': [0, 1],\n",
       "  'cardinality': 2,\n",
       "  'pos_weight': 0.1210527319114325},\n",
       " 'AVARIYA_enc': {'classes': [0, 1],\n",
       "  'cardinality': 2,\n",
       "  'pos_weight': 62.16002864047257}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cbbe24be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBCATEGORY_clean\n",
       "Прочее                                       291400\n",
       "Доступ к ИСОД                                252951\n",
       "Запрос на администрирование                  105941\n",
       "Настройка ПО и оборудования                   15032\n",
       "Авария                                        11255\n",
       "Коррекция данных                               7076\n",
       "ОШС                                            5588\n",
       "Консультация                                   4410\n",
       "Программное обеспечение. Региональные ПТК      3776\n",
       "Запрос статуса                                 3575\n",
       "Программное обеспечение                        1817\n",
       "СПГУ                                           1736\n",
       "Запрос на доработку                             925\n",
       "Оборудование                                    205\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SUBCATEGORY_clean'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e430741",
   "metadata": {},
   "source": [
    "### Сохраняем фичи, таргеты и векторизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa405f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(\"../data/features/question_tfidf.npz\", features=X_question_full, ids=df.index.values)\n",
    "np.savez_compressed('../data/features/oe_cat_features.npz', features=X_cat_oe, ids=df.index.values)\n",
    "np.savez_compressed('../data/features/bin_cat_features.npz', features=X_cat_bin, ids=df.index.values)\n",
    "np.savez_compressed('../data/features/time_features.npz', features=X_time, ids=df.index.values)\n",
    "\n",
    "y.to_csv('../data/targets/targets.csv', index=False)\n",
    "\n",
    "# joblib.dump(tfidf, '../models/vectorizers/tfidf_vectorizer.pkl')\n",
    "# for col, le in label_encoders.items():\n",
    "#     joblib.dump(le, f'../models/vectorizers/label_encoder_{col}.pkl')\n",
    "joblib.dump(le, f'../models/vectorizers/SUBCATEGORY_clean_lenc.pkl')\n",
    "\n",
    "for col, oe in ord_encoders.items():\n",
    "    joblib.dump(oe, f'../models/vectorizers/{col}_oenc.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
